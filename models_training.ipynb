{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shallow CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa6gqJC1GB-h"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhDCUyY7EQ9t"
      },
      "source": [
        "# Import libraries\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import scipy.misc \n",
        "import tensorflow\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from math import sqrt\n",
        "from IPython.display import display\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.regularizers import l1, l2\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "import itertools\n",
        "import h5py\n",
        "import cv2\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D10mdmbGJoe"
      },
      "source": [
        "Set seeding to ensure consistent result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71CjUAw2GSaA"
      },
      "source": [
        "# produce stable results\n",
        "from numpy.random import seed \n",
        "seed(1)\n",
        "from tensorflow import random\n",
        "tensorflow.random.set_seed(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnHGeiXHGVFq"
      },
      "source": [
        "Define paths\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl4t4VjcGZ8L"
      },
      "source": [
        "# define dataset path \n",
        "dataset_path = \"/content/drive/My Drive/Colab Notebooks/FER2013/fer2013.csv\"\n",
        "# define weight saving path\n",
        "weight_path=\"/content/drive/My Drive/Colab Notebooks/FER2013/model02/weights_min_loss.hdf5\"\n",
        "# define model saving path\n",
        "model_path = \"/content/drive/My Drive/Colab Notebooks/FER2013/model02/FER2013.hdf5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8evp-DDJDgP"
      },
      "source": [
        "Read dataset and divide to train/validation/test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPqVjuYiJLwc"
      },
      "source": [
        "# read dataset\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# splitting for training set\n",
        "train_data = data[(data.Usage == \"Training\")]\n",
        "# splitting for validation set\n",
        "val_data = data[(data.Usage == \"PublicTest\")]\n",
        "# splitting for test set\n",
        "test_data = data[(data.Usage == \"PrivateTest\")]\n",
        "\n",
        "#convert to 2-Dimensional ndarray n(48*48)\n",
        "X_train = np.array(list(map(str.split, train_data.pixels)), np.uint8)\n",
        "X_val = np.array(list(map(str.split, val_data.pixels)), np.float32)\n",
        "X_test = np.array(list(map(str.split, test_data.pixels)), np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BUwcqN2LYld"
      },
      "source": [
        "Histogram Equalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aVV-F8JLeMy"
      },
      "source": [
        "#HE processing on training set\n",
        "def he_preprocessing(img: np.ndarray):\n",
        "  list = []\n",
        "  for rows in img:\n",
        "    he_img = cv2.equalizeHist(rows)\n",
        "    list.append(he_img)\n",
        "  \n",
        "  img_set = np.array(list)\n",
        "\n",
        "  return img_set\n",
        "\n",
        "X_train = clahe_preprocessing(X_train)\n",
        "#convert to float32 for data augmentation\n",
        "X_train = np.array(X_train, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxCtU_ntLnPk"
      },
      "source": [
        "OR Contrast Limited Adaptive Histogram Equalization (CLAHE)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awi2UD7-Lsiu"
      },
      "source": [
        "#CLAHE processing on training set\n",
        "def clahe_preprocessing(img: np.ndarray):\n",
        "  list = []\n",
        "  for rows in img:\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(width,height))\n",
        "    clahe_img = clahe.apply(rows)\n",
        "    list.append(clahe_img)\n",
        "  \n",
        "  img_set = np.array(list)\n",
        "\n",
        "  return img_set\n",
        "\n",
        "X_train = clahe_preprocessing(X_train)\n",
        "\n",
        "# convert to float32 for data augmentation\n",
        "X_train = np.array(X_train, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2wMQw9CMHs_"
      },
      "source": [
        "Data Augmentation and Min-max Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMvoMDcwMMuf"
      },
      "source": [
        "# reshape data to 4-dimensional array (n,48,48,1) where n = number of samples\n",
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
        "X_val = X_val.reshape(X_val.shape[0], 48, 48, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n",
        "\n",
        "# get number of samples in each subset\n",
        "num_train = X_train.shape[0]\n",
        "num_val = X_val.shape[0]\n",
        "num_test = X_test.shape[0]\n",
        "\n",
        "# reshaping dataset\n",
        "y_train = train_data.emotion\n",
        "y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "y_val = val_data.emotion\n",
        "y_val = np_utils.to_categorical(y_val, num_classes)\n",
        "y_test = test_data.emotion\n",
        "y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Data augmentation for training data \n",
        "data_gen = ImageDataGenerator(\n",
        "    rescale=1./255, # min-max normalization\n",
        "    fill_mode = 'nearest',\n",
        "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=True  # randomly flip images\n",
        "    )\n",
        "\n",
        "# Data augmentation for test data (only min-max normalization)\n",
        "test_gen = ImageDataGenerator( \n",
        "    rescale=1./255 # min-max normalization\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg0-qvxFNInf"
      },
      "source": [
        "Machine Learning Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVaqSegYNMj0"
      },
      "source": [
        "# define batch size\n",
        "batch_size = 128\n",
        "# define number of epoch\n",
        "num_epochs = 200\n",
        "# define number of classes\n",
        "num_classes = 7\n",
        "# define emotion labels\n",
        "emotion_labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
        "classes = np.array((\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"))\n",
        "# define input shape\n",
        "width, height = 48, 48\n",
        "input_shape = (width, height, 1) # 1 indicates single channel = greyscale\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXP3dUTVN4jN"
      },
      "source": [
        "Define Shallow CNN Architectures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl5l9KTTPVhI"
      },
      "source": [
        "def FER_model(input_shape = (48,48,1)):\n",
        "    #first input model\n",
        "    visible = Input(shape = input_shape, name='input')\n",
        "    \n",
        "\n",
        "    #the first block\n",
        "    conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_1')(visible)\n",
        "    conv1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_2')(conv1_1)\n",
        "    conv1_2 = BatchNormalization()(conv1_2)\n",
        "    pool1_1 = MaxPooling2D(pool_size=(2,2), name = 'pool1_1')(conv1_2)\n",
        "    drop1_1 = Dropout(0.3, name = 'drop1_1')(pool1_1)\n",
        "\n",
        "    #the 2-nd block\n",
        "    conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_1')(drop1_1)\n",
        "    conv2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_2')(conv2_1)\n",
        "    conv2_2 = BatchNormalization()(conv2_2)\n",
        "    conv2_3 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_3')(conv2_2)\n",
        "    conv2_2 = BatchNormalization()(conv2_3)\n",
        "    pool2_1 = MaxPooling2D(pool_size=(2,2), name = 'pool2_1')(conv2_3)\n",
        "    drop2_1 = Dropout(0.3, name = 'drop2_1')(pool2_1)\n",
        "\n",
        "    #the 3-rd block\n",
        "    conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_1')(drop2_1)\n",
        "    conv3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_2')(conv3_1)\n",
        "    conv3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_3')(conv3_2)\n",
        "    conv3_3 = BatchNormalization()(conv3_3)\n",
        "    conv3_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_4')(conv3_3)\n",
        "    conv3_4 = BatchNormalization()(conv3_4)\n",
        "    pool3_1 = MaxPooling2D(pool_size=(2,2), name = 'pool3_1')(conv3_4)\n",
        "    drop3_1 = Dropout(0.3, name = 'drop3_1')(pool3_1)\n",
        "\n",
        "    #the 4-th block\n",
        "    conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_1')(drop3_1)\n",
        "    conv4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_2')(conv4_1)\n",
        "    conv4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_3')(conv4_2)\n",
        "    conv4_3 = BatchNormalization()(conv4_3)\n",
        "    conv4_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_4')(conv4_3)\n",
        "    conv4_4 = BatchNormalization()(conv4_4)\n",
        "    pool4_1 = MaxPooling2D(pool_size=(2,2), name = 'pool4_1')(conv4_4)\n",
        "    drop4_1 = Dropout(0.3, name = 'drop4_1')(pool4_1)\n",
        "\n",
        "    #the 5-th block\n",
        "    conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_1')(drop4_1)\n",
        "    conv5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_2')(conv5_1)\n",
        "    conv5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_3')(conv5_2)\n",
        "    conv5_3 = BatchNormalization()(conv5_3)\n",
        "    conv5_4 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_4')(conv5_3)\n",
        "    conv5_3 = BatchNormalization()(conv5_3)\n",
        "    pool5_1 = MaxPooling2D(pool_size=(2,2), name = 'pool5_1')(conv5_4)\n",
        "    drop5_1 = Dropout(0.3, name = 'drop5_1')(pool5_1)\n",
        "\n",
        "    #Flatten and output\n",
        "    flatten = Flatten(name = 'flatten')(drop5_1)\n",
        "    output = Dense(num_classes, activation='softmax', name = 'output')(flatten)\n",
        "\n",
        "    # create model \n",
        "    model = Model(inputs =visible, outputs = output)\n",
        "    # summary layers\n",
        "    print(model.summary())\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xssg7rFok8Wg"
      },
      "source": [
        "Define Xception"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1CRTMQ0lGx7"
      },
      "source": [
        "def entry_flow(inputs) :\n",
        "    \n",
        "    x = Conv2D(32, 3, strides = 2, padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    x = Conv2D(64,3,padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    previous_block_activation = x\n",
        "    \n",
        "    for size in [64, 128, 256] :\n",
        "    \n",
        "        x = Activation('relu')(x)\n",
        "        x = SeparableConv2D(size, 3, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "    \n",
        "        x = Activation('relu')(x)\n",
        "        x = SeparableConv2D(size, 3, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        \n",
        "        x = MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "        \n",
        "        residual = Conv2D(size, 1, strides=2, padding='same')(previous_block_activation)\n",
        "        \n",
        "        x = tensorflow.keras.layers.Add()([x, residual])\n",
        "        previous_block_activation = x\n",
        "    \n",
        "    return x\n",
        "\n",
        "def middle_flow(x, num_blocks=8) :\n",
        "    \n",
        "    previous_block_activation = x\n",
        "    \n",
        "    for _ in range(num_blocks) :\n",
        "    \n",
        "        x = Activation('relu')(x)\n",
        "        x = SeparableConv2D(256, 3, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "    \n",
        "        x = Activation('relu')(x)\n",
        "        x = SeparableConv2D(256, 3, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        \n",
        "        x = Activation('relu')(x)\n",
        "        x = SeparableConv2D(256, 3, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        \n",
        "        x = tensorflow.keras.layers.Add()([x, previous_block_activation])\n",
        "        previous_block_activation = x\n",
        "    \n",
        "    return x\n",
        "\n",
        "def exit_flow(x) :\n",
        "    \n",
        "    previous_block_activation = x\n",
        "    \n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(256, 3, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(1024, 3, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "    \n",
        "    residual = Conv2D(1024, 1, strides=2, padding='same')(previous_block_activation)\n",
        "    x = tensorflow.keras.layers.Add()([x, residual])\n",
        "      \n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(728, 3, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(1024, 3, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    x = Dense(7, activation='linear', activity_regularizer=l2(0.001))(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "outputs = exit_flow(middle_flow(entry_flow(inputs)))\n",
        "xception = Model(inputs, outputs)\n",
        "xception.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvL36OH5lUYa"
      },
      "source": [
        "Define MobileNetV2 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kus0GFZlcgo"
      },
      "source": [
        "model = keras.applications.MobileNetV2(weights=None, include_top=False, input_shape=input_shape)\n",
        "x = model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.7)(x)\n",
        "predictions = Dense(num_classes, activation = 'softmax')(x)\n",
        "model = Model(inputs = model.input, outputs = predictions)\n",
        "opt = Adam(lr=0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-7, decay = 1e-6)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer = opt, metrics = [\"acc\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1X1n5Y5lkd0"
      },
      "source": [
        "Define ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSGMyzhOlrus"
      },
      "source": [
        "model = keras.applications.resnet.ResNet50(weights=None, include_top=False, input_shape=input_shape)\n",
        "x = model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(num_classes, activation = 'softmax')(x)\n",
        "model = Model(inputs = model.input, outputs = predictions)\n",
        "opt = Adam(lr=0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-7, decay = 1e-6)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer = opt, metrics = [\"acc\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHAwxeNCltgZ"
      },
      "source": [
        "Define InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGXdAwO6l1A0"
      },
      "source": [
        "model = keras.applications.inception_v3.InceptionV3(weights=None, include_top=False, input_shape=inputs)\n",
        "x = model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')\n",
        "predictions = Dense(num_classes, activation = 'softmax')(x)\n",
        "model = Model(inputs = model.input, outputs = predictions)\n",
        "opt = Adam(lr=0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-7, decay = 1e-6)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer = opt, metrics = [\"acc\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_gZt2SIl6fS"
      },
      "source": [
        "Define EfficientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfGAsrtZmAG8"
      },
      "source": [
        "import efficientnet.keras as efn \n",
        "\n",
        "model = efn.EfficientNetB0(weights=None, include_top=False, input_shape=input_shape)\n",
        "x = model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.7)(x)\n",
        "predictions = Dense(num_classes, activation = 'softmax')(x)\n",
        "model = Model(inputs = model.input, outputs = predictions)\n",
        "opt = Adam(lr=0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-7, decay = 1e-6)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer = opt, metrics = [\"acc\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVz87waFPYhb"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1FzVuAMPbOs"
      },
      "source": [
        "# takes data & label arrays, generate batches of augmented data\n",
        "train_flow = data_gen.flow(X_train, y_train, batch_size=batch_size) \n",
        "val_flow = test_gen.flow(X_val, y_val, batch_size=batch_size) \n",
        "test_flow = test_gen.flow(X_test, y_test, batch_size=batch_size) \n",
        "\n",
        "# compile CNN model with the optimizer, loss function, and metrics as accuracy\n",
        "model = FER_model()\n",
        "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc']) \n",
        "\n",
        "# record model history\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# start the training\n",
        "history = model.fit_generator(train_flow, \n",
        "                    steps_per_epoch=len(X_train) / batch_size, \n",
        "                    epochs=num_epochs,  \n",
        "                    verbose=2,  \n",
        "                    callbacks=callbacks_list,\n",
        "                    validation_data=val_flow,  \n",
        "                    validation_steps=len(X_val) / batch_size)\n",
        "\n",
        "# record training and validation loss\n",
        "train_loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "train_acc=history.history['acc']\n",
        "val_acc=history.history['val_acc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeVsrO9IRSXy"
      },
      "source": [
        "Print losses and plot loss graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUUH6hQpRU1g"
      },
      "source": [
        "# get epochs\n",
        "epochs = range(len(train_acc))\n",
        "\n",
        "# plot train loss\n",
        "plt.plot(epochs,train_loss,'r', label='train_loss')\n",
        "plt.plot(epochs,val_loss,'b', label='val_loss')\n",
        "plt.title('train_loss vs val_loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.savefig('loss.png')\n",
        "\n",
        "# plot validation loss\n",
        "plt.plot(epochs,train_acc,'r', label='train_acc')\n",
        "plt.plot(epochs,val_acc,'b', label='val_acc')\n",
        "plt.title('train_acc vs val_acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('acc')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.savefig('accuracy.png')\n",
        "\n",
        "loss = model.evaluate_generator(test_flow, steps=len(X_test) / batch_size) \n",
        "print(\"Test Loss \" + str(loss[0]))\n",
        "print(\"Test Acc: \" + str(loss[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-9AhRI0Rs5R"
      },
      "source": [
        "Save trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHwoUoA1RuO0"
      },
      "source": [
        "# save trained shallow CNN model\n",
        "model.save(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFKMnFMgRz7f"
      },
      "source": [
        "Plot confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdWD-hP5R21K"
      },
      "source": [
        "def plot_confusion_matrix(y_test, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title='Unnormalized confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    if normalize:\n",
        "        cm = np.round(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], 2)\n",
        "        \n",
        "    np.set_printoptions(precision=2)\n",
        "        \n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    thresh = cm.min() + (cm.max() - cm.min()) / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True expression')\n",
        "    plt.xlabel('Predicted expression')\n",
        "    plt.show()\n",
        "    plt.savefig('cm.png')\n",
        "\n",
        "y_pred_ = model.predict(X_test/255., verbose=1)\n",
        "y_pred = np.argmax(y_pred_, axis=1)\n",
        "t_te = np.argmax(y_test, axis=1)\n",
        "\n",
        "fig = plot_confusion_matrix(y_test=t_te, y_pred=y_pred,\n",
        "                      classes=classes,\n",
        "                      normalize=True,\n",
        "                      cmap=plt.cm.Greys,\n",
        "                      title='Average accuracy: ' + str(np.sum(y_pred == t_te)/len(t_te)) + '\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}